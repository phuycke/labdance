#' @title recovery
#'
#' @description Optimization function that attempts to find the parameter set that
#'    fits the passed data best. Automatically determines whether passed data comes
#'    from a dynamic or non dynamic model. Minimizes the (summed) likelihood using
#'    a box constraints approach ("L-BFGS-B" algorithm (Byrd et al., 1995)).
#' @param base_par The parameters that need to be optimized. For instance, when
#'    all parameters in the dynamic neural LBA need to be optimized, c("a", "b",
#'    "t0", "sd") would need to be passed to base_par.
#' @param df The dataset containing the data used in optimization. Contains either
#'     behavioral data (reaction times, choice), neural data or both.
#' @param cycles The amount of 'tries' that the optimization method has. More
#'     precisely, recovery will attempt MLE for a total of cycles tries.
#' @param sigma_gen Optional: only needed in neural models.
#'     The variance in the generation of the true neural data.
#' @param sigma_mod Optional: only needed in neural models.
#'     The assumed variance of the generation of the neural data, since the
#'     true value (sigma_gen) is typically unknown in real life applications.
#'
#' @return numeric containing named values
#' @examples
#' true      = param_draw(dynamic = T)
#' simulated = simulate.data(true_pars = true,
#'                           sigma_gen = 0.01)
#'
#' recov = recovery(base_par  = c("a", "b", "t0", "sd", "beta"),
#'                  df        = simulated,
#'                  cycles    = 500,
#'                  sigma_mod = 0.01)
#'
#' print(rbind(true, recov[1:(length(recov)-2)]))
#'
#' #             a        b        t0        sd      beta
#' # true 0.1282006 1.121690 0.4681826 0.3257557 0.6642583
#' #      0.3477806 1.200789 0.5587487 0.3394791 0.6684803
#'
#' @export
#' @import rtdists


recovery <- function(base_par,
                     df,
                     cycles    = 500,
                     sigma_gen = NULL,
                     sigma_mod = NULL){

  # determine the type of data
  if ("mean_v1" %in% colnames(df)){
    dynamic = T
    n_drift = NULL
  } else{
    dynamic = F
    n_drift = length(unique(df$repetition))
  }

  # actual parameter recovery
  for (q in 1:cycles){
    o = tryCatch(optim(param_draw(base_par = base_par,                     # initial parameter guess
                                  n_drift  = n_drift,
                                  dynamic  = dynamic),
                       likelihood.summed,                                  # goal function to optimize
                       method        = "L-BFGS-B",                         # minimization method
                       rt            = df$rt,
                       response      = df$response,
                       conditions    = df$repetition,
                       wr            = df$weight_reset,
                       neural_dat    = df$neural,
                       netinput      = df$mean_v1,
                       sigma_mod     = sigma_mod,
                       lower         = rep(0, times = length(start)),      # parameter lower bound
                       upper         = rep(Inf, times = length(start)),    # parameter upper bound
                       control       = list(maxit = 5000),
                       hessian       = TRUE),
                 error = function(e){NA})
    # if o is not NA, AND if converged, AND if the LR estimate != 0 --> break
    if (length(o) > 1){
      if(o$convergence == 0){
        if(!any(o$par < 0.0001)){
          break
        }
      }
    }
  }
  if (length(o) == 1){
    return(rep(NA, times = length(start) + 2))
  } else{
    # save the lowest value of the eigen values of the Hessian
    # all should be positive (> epsilon, with epsilon = 0.01) when minimizing
    h = min(eigen(o$hessian)$values)
    return(c(o$par, o$value, h))
  }
}
