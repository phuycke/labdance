mean_v1 = to_optim[[grep(sprintf("v_%d", i), names(to_optim))]],
mean_v2 = 1-to_optim[[grep(sprintf("v_%d", i), names(to_optim))]],
sd_v2   = to_optim[["sd"]])
} else{
par = c(A       = to_optim[["a"]],
b       = to_optim[["b"]],
t0      = to_optim[["t0"]],
mean_v1 = F,
mean_v2 = F,
sd_v2   = to_optim[["sd"]])
}
spar = par[!grepl("[12]$", names(par))]
# distribution parameters
dist_par_names  = unique(sub("[12]$", "", grep("[12]$", names(par), value = TRUE)))
dist_par        = vector("list",
length = length(dist_par_names))
names(dist_par) = dist_par_names
for (j in dist_par_names){
dist_par[[j]] = as.list(unname(par[grep(j, names(par))]))
}
# set common sd's
dist_par$sd_v = c(dist_par$sd_v, dist_par$sd_v)
if (!is.null(dataset)){
# compute netinputs based on the fed in learning rate, and use this as drift rates
dist_par$mean_v = netinputs(beta = to_optim[[grep("beta", names(to_optim))]],
dataset = dataset)
}
# get summed log-likelihood
if (length(conditions) == 1){
react = list(rt)
resp  = list(response)
} else{
react = list(rt[conditions == i])
resp  = list(response[conditions == i])
}
d = do.call(dLBA, args = c(rt           = react,
response     = resp,
spar,
dist_par,
distribution = "norm",
silent       = TRUE))
# get -loglik for this subsection of the data
if (any(d < 0e-10)){
ll = 1e6
}
else{
ll = -sum(log(d))
}
sum_ll = sum_ll + ll
}
return(sum_ll)
}
likelihood.neural <- function(to_optim, neural_data, conditions = NULL, netinput = NULL){
stopifnot((!is.null(conditions) & is.null(netinput)) | (is.null(conditions) & !is.null(netinput)))
if (!is.null(conditions)){
# for nLBA
sum_ll = 0
for (i in seq_along(unique(conditions))){
sum_ll = sum_ll + sum((neural_data[conditions == i] - to_optim[[grep(sprintf("v_%d", i), names(to_optim))]])^2)
}
return(sum_ll)
} else{
# for dnLBA
return(sum((neural_data - netinput)^2))
}
}
likelihood.summed <- function(to_optim, rt, response, neural_data = NULL,
conditions = NULL, dataset = NULL,
partial_data = NULL, netinput = NULL, sigma_mod = NULL){
ll.behavioral = negloglik.behavioral(to_optim, rt, response, conditions,
dataset)
# for non neural data, only return the behavioral loglikelihood
if (is.null(neural_data)){
return(ll.behavioral)
} else{
# for neural data, return the sum of both
ll.neural = likelihood.neural(to_optim, neural_data, conditions, netinput)
return(ll.behavioral + (1/(2*(sigma_mod)^2)) * ll.neural)
}
}
recovery <- function(base_par, df, cycles, sigma_gen = NULL, sigma_mod = NULL){
# determine the type of data
if ("beta" %in% base_par){
dynamic = T
n_drift = NULL
} else{
dynamic = F
n_drift = length(unique(df$repetition))
}
# actual parameter recovery
for (q in 1:cycles){
o = tryCatch(optim(param.draw(base_par = base_par,                     # initial parameter guess
n_drift  = n_drift,
dynamic  = dynamic),
likelihood.summed,                                  # goal function to optimize
method         = "L-BFGS-B",                         # minimization method
rt             = df$rt,
response       = df$response,
conditions     = df$repetition,
dataset        = df,
neural_dat     = df$neural,
netinput       = df$mean_v1,
sigma_mod      = sigma_mod,
lower          = rep(0, times = length(start)),      # parameter lower bound
upper          = rep(Inf, times = length(start)),    # parameter upper bound
control        = list(maxit = 5000),
hessian        = TRUE),
error = function(e){NA})
# if o is not NA, AND if converged, AND if the LR estimate != 0 --> break
if (length(o) > 1){
if(o$convergence == 0){
if(!any(o$par < 0.0001)){
break
}
}
}
}
if (length(o) == 1){
return(rep(NA, times = length(start) + 2))
} else{
# save the lowest value of the eigen values of the Hessian
# all should be positive (> epsilon, with epsilon = 0.01) when minimizing
h = min(eigen(o$hessian)$values)
return(c(o$par, o$value, h))
}
}
# cycle over subjects ----
library(reticulate)
library(dplyr)
np = import("numpy")
DATA1 = "C:/Users/pieter/Downloads/GitHub/phuycke/PhD-code/Modeling/LBA/Data/Eight drift rates/Datasets/behavioral_data_study_1"
files = list.files(DATA1, pattern = "*.npy")
recov_list = list()
f = files[1]
# extract the ID of the subject
subject = as.numeric(substr(f, 5 ,6))
# load data file and mutate
s  = np$load(file = file.path(DATA1, f))
s  = data.frame(s)
colnames(s) = c("stim", "repetition", "condition", "target", "response",
"accuracy", "rt")
s = s %>% mutate(condition = case_when(condition == 0 ~ "novel",
condition == 1 ~ "repeating"),
target    = target + 1,
response  = response + 1,
rt        = rt / 1000) %>%
mutate(trialnum  = rep(1:32, times = 16),
block     = rep(1:16, each = 32),
response  = ifelse(response == 0, 1, response)) %>%
select(block, trialnum, stim, condition, rt, response)
# recode stimuli to 1,2,3,4
so = c()
for (i in unique(s$block)){
so = c(so, scales::rescale(s[s$block == i, ]$stim, to = c(1, 4)))
}
s$stim = so
# test recovery
recov = recovery(base_par       = c("a", "b", "t0", "sd", "beta"),
df             = s,
cycles         = 500,
sigma_gen      = NULL,
sigma_mod      = NULL)
head(s)
DATA1 = "C:/Users/pieter/Downloads/GitHub/phuycke/PhD-code/Modeling/LBA/Data/Eight drift rates/Datasets/behavioral_data_study_1"
files = list.files(DATA1, pattern = "*.npy")
files[1]
f = files[1]
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
# check how param.draw reacts on faulty input
test_that("param.draw handles faulty input correctly", {
expect_error(param.draw(base_par = c(1, "a", "b"),
n_drift = 8,
dynamic = F))
expect_error(param.draw(base_par = c("a", "b", "standard error"),
n_drift = 8,
dynamic = F))
expect_error(param.draw(base_par = c(),
n_drift = 8,
dynamic = F))
expect_error(param.draw(base_par = c("a", "a", "a"),
n_drift = 8,
dynamic = F))
expect_error(param.draw(n_drift = 8,
dynamic = T))
expect_error(param.draw(n_drift = NULL,
dynamic = F))
expect_error(param.draw(n_drift = "18",
dynamic = F))
expect_error(param.draw(n_drift = 8,
dynamic = "yes"))
expect_error(param.draw(base_par = c("a", "b", "beta"),
n_drift = 8,
dynamic = F))
})
# generate input for the four different models
test_that("the functions provides decent input for each of the four models", {
# two cases for the simple LBA / nLBA
expect_length(param.draw(base_par = c("a", "b", "t0"),
n_drift = 8,
dynamic = F),
11)
expect_length(param.draw(n_drift  = 4,
dynamic  = F),
8)
# three cases for the dLBA / dnLBA
expect_length(param.draw(base_par = c("a"),
dynamic = T),
1)
expect_length(param.draw(dynamic = T),
4)
expect_length(param.draw(base_par = c("a", "b", "t0", "sd", "beta"),
dynamic = T),
5)
})
library(testthat)
# check how param.draw reacts on faulty input
test_that("param.draw handles faulty input correctly", {
expect_error(param.draw(base_par = c(1, "a", "b"),
n_drift = 8,
dynamic = F))
expect_error(param.draw(base_par = c("a", "b", "standard error"),
n_drift = 8,
dynamic = F))
expect_error(param.draw(base_par = c(),
n_drift = 8,
dynamic = F))
expect_error(param.draw(base_par = c("a", "a", "a"),
n_drift = 8,
dynamic = F))
expect_error(param.draw(n_drift = 8,
dynamic = T))
expect_error(param.draw(n_drift = NULL,
dynamic = F))
expect_error(param.draw(n_drift = "18",
dynamic = F))
expect_error(param.draw(n_drift = 8,
dynamic = "yes"))
expect_error(param.draw(base_par = c("a", "b", "beta"),
n_drift = 8,
dynamic = F))
})
# generate input for the four different models
test_that("the functions provides decent input for each of the four models", {
# two cases for the simple LBA / nLBA
expect_length(param.draw(base_par = c("a", "b", "t0"),
n_drift = 8,
dynamic = F),
11)
expect_length(param.draw(n_drift  = 4,
dynamic  = F),
8)
# three cases for the dLBA / dnLBA
expect_length(param.draw(base_par = c("a"),
dynamic = T),
1)
expect_length(param.draw(dynamic = T),
4)
expect_length(param.draw(base_par = c("a", "b", "t0", "sd", "beta"),
dynamic = T),
5)
})
?stopifnot
rm(list = ls())
require(labdance)
set.seed(2022)
# dynamic LBA
true = param.draw(base_par = c("a", "b", "t0", "sd", "beta"),
n_drift  = NULL,
dynamic  = T)
simulated = simulate.data(true_pars = true,
dataset   = NULL)
class(simulated)
true_pars = true
true
dataset   = NULL
xor(is.null(true_pars), is.null(dataset))
true_pars = NULL
xor(is.null(true_pars), is.null(dataset))
colnames(simulated)
rm(list = ls())
require(labdance)
set.seed(2022)
# dynamic LBA
true = param.draw(base_par = c("a", "b", "t0", "sd"),
n_drift  = 8,
dynamic  = F)
simulated = simulate.data(true_pars = true,
dataset   = NULL)
View(simulated)
dataset = simulated
all(c("stim", "repetition", "block_nr") %in% colnames(dataset))
names(c(1, 2, 3))
# test for bad input
test_that("param.draw handles faulty input correctly", {
expect_error(param.draw(base_par = c(1, "a", "b"),
n_drift = 8,
dynamic = F))
})
true_pars = param.draw(base_par = c("a", "b", "t0", "sd"),
n_drift = 8,
dynamic = NULL)
true_pars = param.draw(base_par = c("a", "b", "t0", "sd"),
n_drift  = 8,
dynamic  = F)
devtools::document()
true_pars
expect_success(simulate.neural(sub_id    = 1,
n_blocks  = 16,
true_pars = true_pars,
sigma_gen = 0.01,
dataset   = NULL))
length(true_pars)
expect_success(simulate.neural(sub_id    = 1,
n_blocks  = 16,
true_pars = true_pars,
sigma_gen = 0.01,
dataset   = NULL))
devtools::document()
expect_success(simulate.neural(sub_id    = 1,
n_blocks  = 16,
true_pars = true_pars,
sigma_gen = 0.01,
dataset   = NULL))
expect_type(simulate.neural(sub_id    = 1,
n_blocks  = 16,
true_pars = true_pars,
sigma_gen = 0.01,
dataset   = NULL),
"data.frame")
simulated = simulate.neural(true_pars = true,
dataset   = NULL,
sigma_gen = 0.01)
typeof(simulated)
class(simulated)
expect_error(simulate.neural(sub_id      = 1,
n_blocks  = 16,
true_pars = NULL,
sigma_gen = 0.01,
dataset   = NULL))
# test for bad input
test_that("param.draw handles faulty input correctly", {
expect_error(simulate.neural(sub_id      = 1,
n_blocks  = 16,
true_pars = NULL,
sigma_gen = 0.01,
dataset   = NULL))
expect_error(simulate.neural(sub_id      = 1,
n_blocks  = 16,
true_pars = NULL,
sigma_gen = 0.01,
dataset   = NULL))
})
devtools::document
devtools::document()
expect_error(simulate.neural(sub_id    = -1,
n_blocks  = 16,
true_pars = NULL,
sigma_gen = 0.01,
dataset   = NULL))
expect_error(simulate.neural(sub_id    = 1,
n_blocks  = -1,
true_pars = NULL,
sigma_gen = 0.01,
dataset   = NULL))
# test for bad input
test_that("param.draw handles faulty input correctly", {
# parameters that can be used to simulate data
true = param.draw(base_par = c("a", "b", "t0", "sd"),
n_drift  = 8,
dynamic  = F)
expect_error(simulate.neural(sub_id      = 1,
n_blocks  = 16,
true_pars = NULL,
sigma_gen = 0.01,
dataset   = NULL))
expect_error(simulate.neural(sub_id    = -1,
n_blocks  = 16,
true_pars = true,
sigma_gen = 0.01,
dataset   = NULL))
expect_error(simulate.neural(sub_id    = 1,
n_blocks  = -1,
true_pars = true,
sigma_gen = 0.01,
dataset   = NULL))
})
expect_error(simulate.neural(sub_id    = 1,
n_blocks  = -1,
true_pars = true,
sigma_gen = -6,
dataset   = NULL))
load("data/sub-02 - simulate.neural.RData")
head(d)
expect_error(simulate.neural(sub_id    = 1,
n_blocks  = 1,
true_pars = true,
sigma_gen = -6,
dataset   = NULL))
d_copy = d
d_copy$stim = NULL
expect_error(simulate.neural(true_pars = true,
sigma_gen = 0.01,
dataset   = d_copy))
d_copy = d
d_copy$stim = NULL
expect_error(simulate.neural(true_pars = true,
sigma_gen = 0.01,
dataset   = d_copy))
d_copy = d
d_copy$repetition = NULL
expect_error(simulate.neural(true_pars = true,
sigma_gen = 0.01,
dataset   = d_copy))
d_copy = d
d_copy$block_nr = NULL
expect_error(simulate.neural(true_pars = true,
sigma_gen = 0.01,
dataset   = d_copy))
# test with dynamic parameters
true = param.draw(base_par = c("a", "b", "t0", "sd", "beta"),
n_drift  = NULL,
dynamic  = T)
expect_error(simulate.neural(true_pars = true,
sigma_gen = 0.01,
dataset   = NULL))
expect_error(simulate.neural(true_pars = NULL,
sigma_gen = 0.01,
dataset   = NULL))
seq(.1, .5, .1)
expect_error(simulate.neural(true_pars = seq(.1, .4, .1),
sigma_gen = 0.01,
dataset   = NULL))
# test for bad input
test_that("param.draw handles faulty input correctly", {
# parameters that can be used to simulate data
true = param.draw(base_par = c("a", "b", "t0", "sd"),
n_drift  = 8,
dynamic  = F)
# test with simulated data
expect_error(simulate.neural(true_pars = NULL,
sigma_gen = 0.01,
dataset   = NULL))
expect_error(simulate.neural(sub_id    = -1,
true_pars = true,
sigma_gen = 0.01,
dataset   = NULL))
expect_error(simulate.neural(n_blocks  = -1,
true_pars = true,
sigma_gen = 0.01,
dataset   = NULL))
expect_error(simulate.neural(true_pars = true,
sigma_gen = -6,
dataset   = NULL))
expect_error(simulate.neural(true_pars = NULL,
sigma_gen = 0.01,
dataset   = NULL))
expect_error(simulate.neural(true_pars = seq(.1, .4, .1),
sigma_gen = 0.01,
dataset   = NULL))
# tests with empirical data
load("data/sub-02 - simulate.neural.RData")
d_copy = d
d_copy$stim = NULL
expect_error(simulate.neural(true_pars = true,
sigma_gen = 0.01,
dataset   = d_copy))
d_copy = d
d_copy$repetition = NULL
expect_error(simulate.neural(true_pars = true,
sigma_gen = 0.01,
dataset   = d_copy))
d_copy = d
d_copy$block_nr = NULL
expect_error(simulate.neural(true_pars = true,
sigma_gen = 0.01,
dataset   = d_copy))
# test with dynamic parameters
true = param.draw(base_par = c("a", "b", "t0", "sd", "beta"),
n_drift  = NULL,
dynamic  = T)
expect_error(simulate.neural(true_pars = true,
sigma_gen = 0.01,
dataset   = NULL))
})
devtools::test()
getwd()
getwd()
devtools::document()
devtools::test()
devtools::document()
devtools::test()
system.file("sub-02 - simulate.neural.RData", package = "labdance")
system.file("sub-02 - simulate.neural", package = "labdance")
devtools::document()
devtools::test()
devtools::document()
devtools::test()
devtools::document()
devtools::test()
system.file("data", "sub-02 - simulate.neural.RData", package="labdance")
devtools::document()
devtools::test()
devtools::document()
devtools::test()
