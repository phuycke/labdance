silent = T)
df$rt[i] = simulated$rt
df$response[i] = simulated$response
# add neural data (a function of the drift rate) is sigma generation is known
if (!is.null(sigma_gen)){
df$neural[i] = rnorm(1, v, sigma_gen)
}
}
# add accuracy condition to check
levels(df$repetition) = as.character(1:8)
return(df)
}
# simulate data ----
data.sim = simulate.neural(sub_id    = 5,
n_blocks  = 8,
true_pars = params,
sigma_gen = NULL,
dataset   = data.true)
View(data.sim)
nrow(data.true)
nrow(data.true)/32
rep(1:(nrow(data.true)/32), each = 32)
df$block_nr = rep(1:(nrow(data.true)/32), each = 32)
simulate.neural <- function(sub_id    = 1,
n_blocks  = 16,
true_pars = NULL,
sigma_gen = 0.01,
dataset   = NULL){
# checking for faulty input
stopifnot(exprs = {
!all(is.null(c(sub_id, n_blocks, true_pars, sigma_gen, dataset)))
all(is.numeric(c(sub_id, n_blocks, sigma_gen)))
(class(dataset) %in% c("NULL", "data.frame"))
length(grep("v_", names(true_pars))) > 0
length(names(true_pars)) == length(unique(names(true_pars)))
all(c("a", "b", "t0", "sd") %in% names(true_pars))
(sub_id > 0)
(sigma_gen > 0 & sigma_gen < 100)
})
if (!is.null(dataset)){
stopifnot(exprs = {
is.data.frame(dataset)
all(c("stim", "repetition") %in% colnames(dataset))
})
} else{
stopifnot(n_blocks > 0)
}
# placeholder for later
df = data.frame(rep(sub_id, times = n_blocks * 32))
colnames(df) = "sub_id"
# determine the stimulus order and associated repetition (simulated or empirical)
if (is.null(dataset)){
df$stim = rep(rep(1:4, times = 8), times = n_blocks)
df$repetition = rep(rep(1:8, each = 4), times = n_blocks)
df$block_nr = rep(1:n_blocks, each = 32)
} else{
df$stim = dataset$stim
df$repetition = dataset$repetition
df$block_nr = rep(1:(nrow(data.true)/32), each = 32)
}
# dependent variables
df$rt = NA
df$response = NA
# neural data (optional)
if (!is.null(sigma_gen)){
df$neural = NA
}
# drift rates positively correlated with repetition count
drifts = true_pars[grep("v_", names(true_pars))]
for (i in 1:nrow(df)){
# get stimulus and associated repetition
stim       = df$stim[i]
repetition = df$repetition[i]
# determine drift rate based on repetition count
v = c(drifts[repetition], 1-drifts[repetition])
# simulate an LBA trial
simulated <- rLBA(1,
A      = true_pars["a"],
b      = true_pars["b"],
t0     = true_pars["t0"],
mean_v = v,
sd_v   = c(true_pars["sd"], true_pars["sd"]),
silent = T)
df$rt[i] = simulated$rt
df$response[i] = simulated$response
# add neural data (a function of the drift rate) is sigma generation is known
if (!is.null(sigma_gen)){
df$neural[i] = rnorm(1, v, sigma_gen)
}
}
# add accuracy condition to check
levels(df$repetition) = as.character(1:8)
return(df)
}
# simulate data ----
data.sim = simulate.neural(sub_id    = 5,
n_blocks  = 8,
true_pars = params,
sigma_gen = NULL,
dataset   = data.true)
View(data.sim)
rm(list = ls())
require(rtdists)
# model functions ----
param.draw <- function(base_par = c("a", "b", "t0", "sd"),
n_drift  = NULL,
dynamic  = F){
# checking for faulty input
stopifnot(exprs = {
all(is.character(base_par))
length(base_par) > 1
length(base_par) == length(unique(base_par))
(class(n_drift) %in% c("numeric", "integer", "NULL"))
is.logical(dynamic)
((is.null(n_drift) & isTRUE(dynamic)) |
(!is.null(n_drift) & isFALSE(dynamic)))
})
if (is.numeric(n_drift)) stopifnot(n_drift > 1)
for (e in base_par) stopifnot(e %in% c("a", "b", "t0", "sd", "beta"))
if (!dynamic){
if("beta" %in% base_par){
stop("/nNon dynamic model but beta is asked.")
}
}
s1 <- c(
a    = runif(1, 0, .75),
b    = runif(1, .75, 1.5),
t0   = runif(1, 0.25, 0.75),
sd   = runif(1, 0, 0.5),
beta = runif(1, 0, 1)
)
if (dynamic){
return(s1[base_par])
} else{
s2 <- sort(rnorm(n_drift, 0.5, 0.1), decreasing = F)
names(s2) <- paste0("v_", seq_len(n_drift))
return(c(s1[base_par], s2))
}
}
simulate.neural <- function(sub_id    = 1,
n_blocks  = 16,
true_pars = NULL,
sigma_gen = 0.01){
# checking for faulty input
stopifnot(exprs = {
!all(is.null(c(sub_id, n_blocks, true_pars, sigma_gen)))
all(is.numeric(c(sub_id, n_blocks, sigma_gen)))
length(true_pars) > 5
length(names(true_pars)) == length(unique(names(true_pars)))
(sub_id > 0 & n_blocks > 0)
(sigma_gen > 0 & sigma_gen < 100)
})
# placeholder for later
all_dat = c()
for (i in 1:n_blocks){
# define a single block
stim       = rep((4*i-3):(4*i), times = 8)
repetition = rep(1:8,     each  = 4)
d          = data.frame(cbind(stim, repetition))
# placeholders and block numbers
d$rt       = -1
d$response = -1
d$block_nr = i
# drift rates positively correlated with repetition count
drifts = true_pars[grep("v_", names(true_pars))]
for (j in 1:nrow(d)){
# determine drift rate based on repetition count and target response
# drift rates denote the probability on a correct and incorrect response
v = c(drifts[d$repetition[j]], 1-drifts[d$repetition[j]])
# simulate an LBA trial
simulated <- rLBA(1,
A      = true_pars["a"],
b      = true_pars["b"],
t0     = true_pars["t0"],
mean_v = v,
sd_v   = c(true_pars["sd"], true_pars["sd"]),
silent = T)
d$rt[j] = simulated$rt
d$response[j] = simulated$response
# add neural data (a function of the drift rate) is sigma generation is known
if (!is.null(sigma_gen)){
d$neural[j] = rnorm(1, v, sigma_gen)
}
}
# add accuracy condition to check
levels(d$repetition) = as.character(1:8)
# bind together
all_dat = rbind(all_dat, d)
}
# add subject number and return
all_dat$subject = sub_id
return(all_dat)
}
simulate.dynamic <- function(true_pars, sigma_gen = NULL){
# four stimuli and their targets
stimuli = diag(4)
target  = matrix(c(1, 1, 0, 0, 0, 0, 1, 1), nrow = 4)
# define weights, dataholder and trial order
w      = matrix(0,
nrow = nrow(stimuli),
ncol = ncol(target))
df     = data.frame(matrix(0,
nrow = 512,
ncol = 8))
# column names (used laeter)
c_names = c("rt", "response", "stimulus", "target",
"accuracy", "weight_reset", "mean_v1", "mean_v2")
# add an extra column for neural data if needed
if (!is.null(sigma_gen)){
df = cbind(df, rep(0, times = nrow(df)))
c_names = c(c_names, "neural")
}
trials = rep(1:nrow(stimuli),
times = 512 / nrow(stimuli))
# count the number of trials since last weight reset
counter    = 1
when_reset = round(rnorm(1, 32, 0))
# for each trial change the weights
for (i in 1:length(trials)){
# determine which stimulus is shown
s = trials[i]
# logistic function where all(sum(netinput) == 1)
netinput = 1 / (1 + exp(-(stimuli[s,] %*% w)))
stopifnot(round(sum(netinput), 10) == 1)
# LBA trial estimation where netinputs to output unit serve as v1 and v2
df[i,1:2] = rLBA(1,
A      = true_pars["a"],
b      = true_pars["b"],
t0     = true_pars["t0"],
mean_v = as.vector(netinput),
sd_v   = c(true_pars["sd"], true_pars["sd"]),
silent = T)
# add neural data
if (!is.null(sigma_gen)){
df[i,9] = rnorm(1, netinput[1], sigma_gen)
}
# weight update
A = matrix(stimuli[s,])
# TODO: dit deel verwijderen: * netinput * (1 - netinput) in lijn met cross entropy error function
# error functie is een bernoulli verdeling (ipv een normaalverdeling zoals we initieel zeiden)
B = (target[s, ] - netinput)
w = w + true_pars["beta"] * (A %*% B)
# save stimulus and associated target
df[i,3] = s
df[i,4] = which.max(target[s,])
# save whether trials was correct or not
if(which.max(target[s,]) == df[i,2]){
df[i,5] = 1
} else{
df[i,5] = 0
}
# when to reverse is drawn from N(m, s), reset W, counter and draw from N(m,s)
if (counter == when_reset){
w = matrix(0, nrow = nrow(stimuli), ncol = ncol(target))
#cat("Weight reset on trial", i, "/n")
df[i,6]    = T
counter    = 0
when_reset = round(rnorm(1, 32, 0))
} else{
df[i,6] = F
}
# save the netinputs (used during estimation of LR) and increment counter for reset
df[i,7:8] = netinput
counter   = counter + 1
}
# change the column names and return the resulting dataframe
colnames(df) = c("rt", "response", "stimulus", "target",
"accuracy", "weight_reset", "mean_v1", "mean_v2")
return(df)
}
simulate.data <-  function(sub_id = NULL, n_blocks = NULL, true_pars, sigma_gen = NULL){
# (n)LBA
if (!"beta" %in% names(true_pars)){
return(simulate.neural(sub_id, n_blocks, true_pars, sigma_gen))
} else{
# d(n)LBA
return(simulate.dynamic(true_pars, sigma_gen))
}
}
netinputs <- function(beta,
dataset){
stim = diag(4)
t    = matrix(c(1, 1, 0, 0, 0, 0, 1, 1),
nrow = 4)
w_nov = matrix(0,
nrow = nrow(stim),
ncol = ncol(t))
w_rep = matrix(0,
nrow = nrow(stim),
ncol = ncol(t))
# data holder
df      = matrix(0,
nrow = nrow(dataset),
ncol = 2)
# add block number and trial number
df = data.frame(df)
colnames(df) = c("mean_v1", "mean_v2")
# simulate
for (i in 1:nrow(dataset)){
s = dataset$stim[i]
# input at output units, logistic activation function
if (dataset$condition[i] == "novel"){
netinput = 1 / (1 + exp(-(stim[s,] %*% w_nov)))
} else{
netinput = 1 / (1 + exp(-(stim[s,] %*% w_rep)))
}
stopifnot(round(sum(netinput), 10) == 1)
# weight update
A = matrix(stim[s,])
B = (t[s, ] - netinput)
# input at output units, logistic activation function
if (dataset$condition[i] == "novel"){
w_nov = w_nov + beta * (A %*% B)
} else{
w_rep = w_rep + beta * (A %*% B)
}
# save the netinputs (used during estimation of LR)
df[i,1:2] = netinput
# reset the weights based on random draw
if (dataset$trialnum[i] == 32 & dataset$condition[i] == "novel"){
w_nov = matrix(0,
nrow = nrow(stim),
ncol = ncol(t))
}
}
# return the actual netinputs
return(list(df$mean_v1, df$mean_v2))
}
negloglik.behavioral <- function(to_optim,
rt,
response,
conditions = NULL,
dataset    = NULL) {
# at least one of the two must be NULL
if (is.null(conditions)){
conditions = 1
}
# summed loglik
sum_ll = 0
for (i in seq_along(unique(conditions))){
# assign parameters to variable names
if (is.null(dataset)){
par = c(A       = to_optim[["a"]],
b       = to_optim[["b"]],
t0      = to_optim[["t0"]],
mean_v1 = to_optim[[grep(sprintf("v_%d", i), names(to_optim))]],
mean_v2 = 1-to_optim[[grep(sprintf("v_%d", i), names(to_optim))]],
sd_v2   = to_optim[["sd"]])
} else{
par = c(A       = to_optim[["a"]],
b       = to_optim[["b"]],
t0      = to_optim[["t0"]],
mean_v1 = F,
mean_v2 = F,
sd_v2   = to_optim[["sd"]])
}
spar = par[!grepl("[12]$", names(par))]
# distribution parameters
dist_par_names  = unique(sub("[12]$", "", grep("[12]$", names(par), value = TRUE)))
dist_par        = vector("list",
length = length(dist_par_names))
names(dist_par) = dist_par_names
for (j in dist_par_names){
dist_par[[j]] = as.list(unname(par[grep(j, names(par))]))
}
# set common sd's
dist_par$sd_v = c(dist_par$sd_v, dist_par$sd_v)
if (!is.null(dataset)){
# compute netinputs based on the fed in learning rate, and use this as drift rates
dist_par$mean_v = netinputs(beta = to_optim[[grep("beta", names(to_optim))]],
dataset = dataset)
}
# get summed log-likelihood
if (length(conditions) == 1){
react = list(rt)
resp  = list(response)
} else{
react = list(rt[conditions == i])
resp  = list(response[conditions == i])
}
d = do.call(dLBA, args = c(rt           = react,
response     = resp,
spar,
dist_par,
distribution = "norm",
silent       = TRUE))
# get -loglik for this subsection of the data
if (any(d < 0e-10)){
ll = 1e6
}
else{
ll = -sum(log(d))
}
sum_ll = sum_ll + ll
}
return(sum_ll)
}
likelihood.neural <- function(to_optim, neural_data, conditions = NULL, netinput = NULL){
stopifnot((!is.null(conditions) & is.null(netinput)) | (is.null(conditions) & !is.null(netinput)))
if (!is.null(conditions)){
# for nLBA
sum_ll = 0
for (i in seq_along(unique(conditions))){
sum_ll = sum_ll + sum((neural_data[conditions == i] - to_optim[[grep(sprintf("v_%d", i), names(to_optim))]])^2)
}
return(sum_ll)
} else{
# for dnLBA
return(sum((neural_data - netinput)^2))
}
}
likelihood.summed <- function(to_optim, rt, response, neural_data = NULL,
conditions = NULL, dataset = NULL,
partial_data = NULL, netinput = NULL, sigma_mod = NULL){
ll.behavioral = negloglik.behavioral(to_optim, rt, response, conditions,
dataset)
# for non neural data, only return the behavioral loglikelihood
if (is.null(neural_data)){
return(ll.behavioral)
} else{
# for neural data, return the sum of both
ll.neural = likelihood.neural(to_optim, neural_data, conditions, netinput)
return(ll.behavioral + (1/(2*(sigma_mod)^2)) * ll.neural)
}
}
recovery <- function(base_par, df, cycles, sigma_gen = NULL, sigma_mod = NULL){
# determine the type of data
if ("beta" %in% base_par){
dynamic = T
n_drift = NULL
} else{
dynamic = F
n_drift = length(unique(df$repetition))
}
# actual parameter recovery
for (q in 1:cycles){
o = tryCatch(optim(param.draw(base_par = base_par,                     # initial parameter guess
n_drift  = n_drift,
dynamic  = dynamic),
likelihood.summed,                                  # goal function to optimize
method         = "L-BFGS-B",                         # minimization method
rt             = df$rt,
response       = df$response,
conditions     = df$repetition,
dataset        = df,
neural_dat     = df$neural,
netinput       = df$mean_v1,
sigma_mod      = sigma_mod,
lower          = rep(0, times = length(start)),      # parameter lower bound
upper          = rep(Inf, times = length(start)),    # parameter upper bound
control        = list(maxit = 5000),
hessian        = TRUE),
error = function(e){NA})
# if o is not NA, AND if converged, AND if the LR estimate != 0 --> break
if (length(o) > 1){
if(o$convergence == 0){
if(!any(o$par < 0.0001)){
break
}
}
}
}
if (length(o) == 1){
return(rep(NA, times = length(start) + 2))
} else{
# save the lowest value of the eigen values of the Hessian
# all should be positive (> epsilon, with epsilon = 0.01) when minimizing
h = min(eigen(o$hessian)$values)
return(c(o$par, o$value, h))
}
}
# cycle over subjects ----
library(reticulate)
library(dplyr)
np = import("numpy")
DATA1 = "C:/Users/pieter/Downloads/GitHub/phuycke/PhD-code/Modeling/LBA/Data/Eight drift rates/Datasets/behavioral_data_study_1"
files = list.files(DATA1, pattern = "*.npy")
recov_list = list()
f = files[1]
# extract the ID of the subject
subject = as.numeric(substr(f, 5 ,6))
# load data file and mutate
s  = np$load(file = file.path(DATA1, f))
s  = data.frame(s)
colnames(s) = c("stim", "repetition", "condition", "target", "response",
"accuracy", "rt")
s = s %>% mutate(condition = case_when(condition == 0 ~ "novel",
condition == 1 ~ "repeating"),
target    = target + 1,
response  = response + 1,
rt        = rt / 1000) %>%
mutate(trialnum  = rep(1:32, times = 16),
block     = rep(1:16, each = 32),
response  = ifelse(response == 0, 1, response)) %>%
select(block, trialnum, stim, condition, rt, response)
# recode stimuli to 1,2,3,4
so = c()
for (i in unique(s$block)){
so = c(so, scales::rescale(s[s$block == i, ]$stim, to = c(1, 4)))
}
s$stim = so
# test recovery
recov = recovery(base_par       = c("a", "b", "t0", "sd", "beta"),
df             = s,
cycles         = 500,
sigma_gen      = NULL,
sigma_mod      = NULL)
head(s)
DATA1 = "C:/Users/pieter/Downloads/GitHub/phuycke/PhD-code/Modeling/LBA/Data/Eight drift rates/Datasets/behavioral_data_study_1"
files = list.files(DATA1, pattern = "*.npy")
files[1]
f = files[1]
