react = list(rt)
resp  = list(response)
} else{
react = list(rt[conditions == i])
resp  = list(response[conditions == i])
}
d = do.call(dLBA, args = c(rt           = react,
response     = resp,
spar,
dist_par,
distribution = "norm",
silent       = TRUE))
# get -loglik for this subsection of the data
if (any(d < 0e-10)){
ll = 1e6
}
else{
ll = -sum(log(d))
}
sum_ll = sum_ll + ll
}
return(sum_ll)
}
likelihood.neural <- function(to_optim, neural_data, conditions = NULL, netinput = NULL){
stopifnot((!is.null(conditions) & is.null(netinput)) | (is.null(conditions) & !is.null(netinput)))
if (!is.null(conditions)){
# for nLBA
sum_ll = 0
for (i in seq_along(unique(conditions))){
sum_ll = sum_ll + sum((neural_data[conditions == i] - to_optim[[grep(sprintf("v_%d", i), names(to_optim))]])^2)
}
return(sum_ll)
} else{
# for dnLBA
return(sum((neural_data - netinput)^2))
}
}
likelihood.summed <- function(to_optim, rt, response, neural_data = NULL, conditions = NULL, wr = NULL,
netinput = NULL, sigma_mod = NULL){
ll.behavioral = negloglik.behavioral(to_optim, rt, response, conditions, wr)
# for non neural data, only return the behavioral loglikelihood
if (is.null(neural_data)){
return(ll.behavioral)
} else{
# for neural data, return the sum of both
ll.neural = likelihood.neural(to_optim, neural_data, conditions, netinput)
return(ll.behavioral + (1/(2*(sigma_mod)^2)) * ll.neural)
}
}
recovery <- function(base_par, df, cycles, sigma_gen = NULL, sigma_mod = NULL){
# determine the type of data
if ("mean_v1" %in% colnames(df)){
dynamic = T
n_drift = NULL
} else{
dynamic = F
n_drift = length(unique(df$repetition))
}
# actual parameter recovery
for (q in 1:cycles){
o = tryCatch(optim(param_draw(base_par = base_par,                     # initial parameter guess
n_drift  = n_drift,
dynamic  = dynamic),
likelihood.summed,                                  # goal function to optimize
method        = "L-BFGS-B",                         # minimization method
rt            = df$rt,
response      = df$response,
conditions    = df$repetition,
wr            = df$weight_reset,
neural_dat    = df$neural,
netinput      = df$mean_v1,
sigma_mod     = sigma_mod,
lower         = rep(0, times = length(start)),      # parameter lower bound
upper         = rep(Inf, times = length(start)),    # parameter upper bound
control       = list(maxit = 5000),
hessian       = TRUE),
error = function(e){NA})
# if o is not NA, AND if converged, AND if the LR estimate != 0 --> break
if (length(o) > 1){
if(o$convergence == 0){
if(!any(o$par < 0.0001)){
break
}
}
}
}
if (length(o) == 1){
return(rep(NA, times = length(start) + 2))
} else{
# save the lowest value of the eigen values of the Hessian
# all should be positive (> epsilon, with epsilon = 0.01) when minimizing
h = min(eigen(o$hessian)$values)
return(c(o$par, o$value, h))
}
}
# testing ----
# LBA
# simulate data based on known parameters
param.lba.true = param_draw(base_par = c("a", "b", "t0", "sd"), n_drift  = 8, dynamic  = F)
simulated.lba  = simulate.data(1, 16, param.lba.true)
# attempt recovery
param.lba.recov = recovery(base_par = c("a", "b", "t0", "sd"),
df       = simulated.lba,
cycles   = 500)
print(rbind(param.lba.true, param.lba.recov[1:(length(param.lba.recov)-2)]))
# dLBA
# simulate data based on known parameters
param.dlba.true = param_draw(dynamic  = T)
simulated.dlba  = simulate.data(true_pars = param.dlba.true)
# attempt recovery
param.dlba.recov = recovery(base_par = c("a", "b", "t0", "sd", "beta"),
df       = simulated.dlba,
cycles   = 500)
print(rbind(param.dlba.true, param.dlba.recov[1:(length(param.dlba.recov)-2)]))
# nLBA
# simulate data based on known parameters
param.nlba.true = param_draw(base_par = c("a", "b", "t0", "sd"), n_drift  = 8, dynamic  = F)
simulated.nlba  = simulate.data(sub_id = 1, n_blocks = 16, true_pars = param.nlba.true, sigma_gen = 0.01)
# attempt recovery
param.nlba.recov = recovery(base_par  = c("a", "b", "t0", "sd"),
df        = simulated.nlba,
cycles    = 500,
sigma_mod = 0.01)
print(rbind(param.nlba.true, param.nlba.recov[1:(length(param.nlba.recov)-2)]))
# dnLBA
# simulate data based on known parameters
param.dnlba.true = param_draw(dynamic  = T)
simulated.dnlba  = simulate.data(true_pars = param.dnlba.true, sigma_gen = 0.01)
# attempt recovery
param.dnlba.recov = recovery(base_par  = c("a", "b", "t0", "sd", "beta"),
df        = simulated.dnlba,
cycles    = 500,
sigma_mod = 0.01)
print(rbind(param.dnlba.true, param.dnlba.recov[1:(length(param.dnlba.recov)-2)]))
installr::updateR()
install.packages("yaml")
library("adaptMCMC", lib.loc="C:/Program Files/R/R-4.2.1/library")
detach("package:adaptMCMC", unload=TRUE)
update.packages(ask = FALSE)
remove.packages("adaptMCMC")
remove.packages("ape")
remove.packages("ash")
remove.packages("bayesplot")
remove.packages("bayestestR")
remove.packages("bridgesampling")
remove.packages("coda")
remove.packages("akima")
remove.packages("crayon")
remove.packages("DEoptim")
remove.packages("DEoptimR")
library("afex", lib.loc="C:/Program Files/R/R-4.2.1/library")
update.packages(checkBuilt = T, ask = F)
rm(list = ls())
require(rtdists)
# model functions ----
param_draw <- function(base_par, n_drift = NULL, dynamic = F) {
stopifnot((is.null(n_drift) & dynamic) | (!is.null(n_drift) & !dynamic))
s1 <- c(
a    = runif(1, 0, .75),
b    = runif(1, .75, 1.5),
t0   = runif(1, 0.25, 0.75),
sd   = runif(1, 0, 0.5),
beta = runif(1, 0, 1)
)
if (dynamic){
return(s1)
} else{
stopifnot(n_drift > 0)
s2 <- sort(rnorm(n_drift, 0.5, 0.1), decreasing = F)
names(s2) <- paste0("v_", seq_len(n_drift))
return(c(s1[base_par], s2))
}
}
simulate.neural <- function(sub_id, n_blocks, true_pars, sigma_gen = NULL){
# placeholder for later
all_dat = c()
for (i in 1:n_blocks){
# define a single block
stim       = rep((4*i-3):(4*i), times = 8)
repetition = rep(1:8,     each  = 4)
d          = data.frame(cbind(stim, repetition))
# placeholders and block numbers
d$rt = -1
d$response = -1
d$block_nr = i
# drift rates positively correlated with repetition count
drifts = true_pars[5:12]
for (j in 1:nrow(d)){
# determine drift rate based on repetition count and target response
# drift rates denote the probability on a correct and incorrect response
v = c(drifts[d$repetition[j]], 1-drifts[d$repetition[j]])
# simulate an LBA trial
simulated <- rLBA(1,
A      = true_pars["a"],
b      = true_pars["b"],
t0     = true_pars["t0"],
mean_v = v,
sd_v   = c(true_pars["sd"], true_pars["sd"]),
silent = T)
d$rt[j] = simulated$rt
d$response[j] = simulated$response
# add neural data (a function of the drift rate) is sigma generation is known
if (!is.null(sigma_gen)){
d$neural[j] = rnorm(1, v, sigma_gen)
}
}
# add accuracy condition to check
levels(d$repetition) = as.character(1:8)
# bind together
all_dat = rbind(all_dat, d)
}
# add subject number and return
all_dat$subject = sub_id
return(all_dat)
}
simulate.dynamic <- function(true_pars, sigma_gen = NULL){
# four stimuli and their targets
stimuli = diag(4)
target  = matrix(c(1, 1, 0, 0, 0, 0, 1, 1), nrow = 4)
# define weights, dataholder and trial order
w      = matrix(0,
nrow = nrow(stimuli),
ncol = ncol(target))
df     = data.frame(matrix(0,
nrow = 512,
ncol = 8))
# column names (used laeter)
c_names = c("rt", "response", "stimulus", "target",
"accuracy", "weight_reset", "mean_v1", "mean_v2")
# add an extra column for neural data if needed
if (!is.null(sigma_gen)){
df = cbind(df, rep(0, times = nrow(df)))
c_names = c(c_names, "neural")
}
trials = rep(1:nrow(stimuli),
times = 512 / nrow(stimuli))
# count the number of trials since last weight reset
counter    = 1
when_reset = round(rnorm(1, 32, 0))
# for each trial change the weights
for (i in 1:length(trials)){
# determine which stimulus is shown
s = trials[i]
# logistic function where all(sum(netinput) == 1)
netinput = 1 / (1 + exp(-(stimuli[s,] %*% w)))
stopifnot(round(sum(netinput), 10) == 1)
# LBA trial estimation where netinputs to output unit serve as v1 and v2
df[i,1:2] = rLBA(1,
A      = true_pars["a"],
b      = true_pars["b"],
t0     = true_pars["t0"],
mean_v = as.vector(netinput),
sd_v   = c(true_pars["sd"], true_pars["sd"]),
silent = T)
# add neural data
if (!is.null(sigma_gen)){
df[i,9] = rnorm(1, netinput[1], sigma_gen)
}
# weight update
A = matrix(stimuli[s,])
# TODO: dit deel verwijderen: * netinput * (1 - netinput) in lijn met cross entropy error function
# error functie is een bernoulli verdeling (ipv een normaalverdeling zoals we initieel zeiden)
B = (target[s, ] - netinput)
w = w + true_pars["beta"] * (A %*% B)
# save stimulus and associated target
df[i,3] = s
df[i,4] = which.max(target[s,])
# save whether trials was correct or not
if(which.max(target[s,]) == df[i,2]){
df[i,5] = 1
} else{
df[i,5] = 0
}
# when to reverse is drawn from N(m, s), reset W, counter and draw from N(m,s)
if (counter == when_reset){
w = matrix(0, nrow = nrow(stimuli), ncol = ncol(target))
#cat("Weight reset on trial", i, "\n")
df[i,6]    = T
counter    = 0
when_reset = round(rnorm(1, 32, 0))
} else{
df[i,6] = F
}
# save the netinputs (used during estimation of LR) and increment counter for reset
df[i,7:8] = netinput
counter   = counter + 1
}
# change the column names and return the resulting dataframe
colnames(df) = c("rt", "response", "stimulus", "target",
"accuracy", "weight_reset", "mean_v1", "mean_v2")
return(df)
}
simulate.data <-  function(sub_id = NULL, n_blocks = NULL, true_pars, sigma_gen = NULL){
# (n)LBA
if (!"beta" %in% names(true_pars)){
return(simulate.neural(sub_id, n_blocks, true_pars, sigma_gen))
} else{
# d(n)LBA
return(simulate.dynamic(true_pars, sigma_gen))
}
}
netinputs <- function(beta, wr){
stim = diag(4)
t    = matrix(c(1, 1, 0, 0, 0, 0, 1, 1),
nrow = 4)
w    = matrix(0,
nrow = nrow(stim),
ncol = ncol(t))
# data holder
df      = matrix(0,
nrow = 512,
ncol = 2)
trials  = rep(1:4,
times = 512 / 4)
# simulate
for (i in 1:length(trials)){
s = trials[i]
# input at output units, logistic activation function
netinput = 1 / (1 + exp(-(stim[s,] %*% w)))
stopifnot(round(sum(netinput), 10) == 1)
# weight update
A = matrix(stim[s,])
B = (t[s, ] - netinput)
w = w + beta * (A %*% B)
# save the netinputs (used during estimation of LR)
df[i,1:2] = netinput
# reset the weights based on random draw
if (wr[i] == 1){
w = matrix(0,
nrow = nrow(stim),
ncol = ncol(t))
}
}
return(list(df[,1], df[,2]))
}
negloglik.behavioral <- function(to_optim, rt, response, conditions = NULL, wr = NULL) {
# at least one of the two must be NULL
stopifnot((!is.null(conditions) & is.null(wr)) | (is.null(conditions) & !is.null(wr)))
if (is.null(conditions)){
conditions = 1
}
# summed loglik
sum_ll = 0
for (i in seq_along(unique(conditions))){
# assign parameters to variable names
if (is.null(wr)){
par = c(A       = to_optim[["a"]],
b       = to_optim[["b"]],
t0      = to_optim[["t0"]],
mean_v1 = to_optim[[grep(sprintf("v_%d", i), names(to_optim))]],
mean_v2 = 1-to_optim[[grep(sprintf("v_%d", i), names(to_optim))]],
sd_v2   = to_optim[["sd"]])
} else{
par = c(A       = to_optim[["a"]],
b       = to_optim[["b"]],
t0      = to_optim[["t0"]],
mean_v1 = F,
mean_v2 = F,
sd_v2   = to_optim[["sd"]])
}
spar = par[!grepl("[12]$", names(par))]
# distribution parameters
dist_par_names  = unique(sub("[12]$", "", grep("[12]$", names(par), value = TRUE)))
dist_par        = vector("list",
length = length(dist_par_names))
names(dist_par) = dist_par_names
for (j in dist_par_names){
dist_par[[j]] = as.list(unname(par[grep(j, names(par))]))
}
# set common sd's
dist_par$sd_v = c(dist_par$sd_v, dist_par$sd_v)
if (!is.null(wr)){
# compute netinputs based on the fed in learning rate, and use this as drift rates
dist_par$mean_v = netinputs(beta = to_optim[[grep("beta", names(to_optim))]],
wr   = wr)
}
# get summed log-likelihood
if (length(conditions) == 1){
react = list(rt)
resp  = list(response)
} else{
react = list(rt[conditions == i])
resp  = list(response[conditions == i])
}
d = do.call(dLBA, args = c(rt           = react,
response     = resp,
spar,
dist_par,
distribution = "norm",
silent       = TRUE))
# get -loglik for this subsection of the data
if (any(d < 0e-10)){
ll = 1e6
}
else{
ll = -sum(log(d))
}
sum_ll = sum_ll + ll
}
return(sum_ll)
}
likelihood.neural <- function(to_optim, neural_data, conditions = NULL, netinput = NULL){
stopifnot((!is.null(conditions) & is.null(netinput)) | (is.null(conditions) & !is.null(netinput)))
if (!is.null(conditions)){
# for nLBA
sum_ll = 0
for (i in seq_along(unique(conditions))){
sum_ll = sum_ll + sum((neural_data[conditions == i] - to_optim[[grep(sprintf("v_%d", i), names(to_optim))]])^2)
}
return(sum_ll)
} else{
# for dnLBA
return(sum((neural_data - netinput)^2))
}
}
likelihood.summed <- function(to_optim, rt, response, neural_data = NULL, conditions = NULL, wr = NULL,
netinput = NULL, sigma_mod = NULL){
ll.behavioral = negloglik.behavioral(to_optim, rt, response, conditions, wr)
# for non neural data, only return the behavioral loglikelihood
if (is.null(neural_data)){
return(ll.behavioral)
} else{
# for neural data, return the sum of both
ll.neural = likelihood.neural(to_optim, neural_data, conditions, netinput)
return(ll.behavioral + (1/(2*(sigma_mod)^2)) * ll.neural)
}
}
recovery <- function(base_par, df, cycles, sigma_gen = NULL, sigma_mod = NULL){
# determine the type of data
if ("mean_v1" %in% colnames(df)){
dynamic = T
n_drift = NULL
} else{
dynamic = F
n_drift = length(unique(df$repetition))
}
# actual parameter recovery
for (q in 1:cycles){
o = tryCatch(optim(param_draw(base_par = base_par,                     # initial parameter guess
n_drift  = n_drift,
dynamic  = dynamic),
likelihood.summed,                                  # goal function to optimize
method        = "L-BFGS-B",                         # minimization method
rt            = df$rt,
response      = df$response,
conditions    = df$repetition,
wr            = df$weight_reset,
neural_dat    = df$neural,
netinput      = df$mean_v1,
sigma_mod     = sigma_mod,
lower         = rep(0, times = length(start)),      # parameter lower bound
upper         = rep(Inf, times = length(start)),    # parameter upper bound
control       = list(maxit = 5000),
hessian       = TRUE),
error = function(e){NA})
# if o is not NA, AND if converged, AND if the LR estimate != 0 --> break
if (length(o) > 1){
if(o$convergence == 0){
if(!any(o$par < 0.0001)){
break
}
}
}
}
if (length(o) == 1){
return(rep(NA, times = length(start) + 2))
} else{
# save the lowest value of the eigen values of the Hessian
# all should be positive (> epsilon, with epsilon = 0.01) when minimizing
h = min(eigen(o$hessian)$values)
return(c(o$par, o$value, h))
}
}
true = param_draw(base_par = c("a", "b", "t0", "sd"), n_drift  = 8, dynamic  = F)
nLBA  = simulate.data(sub_id   = 1,
n_blocks  = 16,
true_pars = true,
sigma_gen = 0.01)
likelihood.neural(true, nLBA$neural, nLBA$repetition)
install.packages("expm")
devtools::document()
install.packages("ellipsis")
devtools::document()
install.packages("rlang")
devtools::document()
install.packages("devtools")
devtools::document()
install.packages("fastmap")
devtools::document()
install.packages("devtools", dependencies = T)
install.packages("devtools", dependencies = T)
devtools::document()
install.packages("prettyunits")
devtools::document()
install.packages("glue")
devtools::document()
install.packages("purrr")
devtools::document()
install.packages("rstudioapi")
devtools::document()
install.packages("stringr")
update.packages(ask = FALSE)
update.packages(ask = FALSE)
devtools::document()
update.packages(oldPkgs = old.packages())
library(waldo)
detach("package:waldo", unload = TRUE)
ip = as.data.frame(installed.packages()[,c(1,3:4)])
ip = ip[is.na(ip$Priority),1:2,drop=FALSE]
ip
ip$Package
update.packages(ask=FALSE, checkBuilt=TRUE)
